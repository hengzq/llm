{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117a235b-4e6b-4a48-b119-78aa84fbc740",
   "metadata": {},
   "source": [
    "# Transformers 微调训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af5a041-d196-4bc4-85d4-af750e22ef8e",
   "metadata": {},
   "source": [
    "- **Transformers 微调步骤**\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph 1.数据处理\n",
    "        direction LR\n",
    "        下载数据集 --> 数据预处理 --> 数据抽样\n",
    "    end\n",
    "    subgraph 2.微调配置\n",
    "        direction LR\n",
    "        训练超参数设置 --> 训练评估指标设置 --> 训练器配置\n",
    "    end\n",
    "    subgraph 3.模型训练\n",
    "        direction LR\n",
    "        开始训练 \n",
    "    end\n",
    "    subgraph 4.保存模型\n",
    "        direction LR\n",
    "        保存模型 --> 保存训练状态\n",
    "    end\n",
    "    \n",
    "    \n",
    "    startNode((开始)):::startClass --> 1.数据处理 --> 2.微调配置 -->  3.模型训练 --> 4.保存模型 --> endNode((结束)):::endClass\n",
    "    classDef startClass fill:#4caf50;\n",
    "    classDef endClass fill:#f44336;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24182c7d-0c87-4006-afc4-954eb51717bd",
   "metadata": {},
   "source": [
    "## 1. 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7570168-a813-47d7-8b15-f9f52402b21d",
   "metadata": {},
   "source": [
    "### 1.1 下载数据集\n",
    "\n",
    "YelpReviewFull 数据集是一个经典的情感分析数据集，包含了大量来自 Yelp 的评论。数据集从 Yelp Dataset Challenge 2015 数据中提取，主要用于文本分类任务，目标是预测评论的情感分数。数据集的评论主要用英语编写，适合进行情感分类研究。\n",
    "\n",
    "- 数据集首页:[https://huggingface.co/datasets/Yelp/yelp_review_full](https://huggingface.co/datasets/Yelp/yelp_review_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb26812-baad-4b55-aadf-b6552f374e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Yelp/yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c2a335-77b0-4a28-9e03-a900948b4891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aca4eaa-6f2b-4eaf-b0b2-58f4ee9135bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 4,\n",
       " 'text': \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d39a08-3764-4191-8ff6-0df7e555cfcc",
   "metadata": {},
   "source": [
    "- 数据随机抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2c057f-45ab-486c-954c-e54e555ee57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples = 10):\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for col, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[col] = df[col].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "800fd24e-622d-4050-bf98-77986a0c407e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Yummy food and nice atmosphere with wonderful service.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Went there last night in hopes of finding some decent Mexican Food. Unfortunately, this was not the case. First of all, the service was lacking. It took us asking twice to just get a container of salt to the table. Also, the waitress was less than interested in helping us with anything more than taking our order. \\n\\nSecond, the food was atrocious. You know you have an issue if you need to add salt to Mexican food. The rice was dry (almost hard and old) and completely tasteless. The beans were bland and tasted like paste. There was barely any cheese in the enchilada I had. My husband's tacos wee haphazardly put together, falling over and impossible to pick up, and needless to say did not taste good. \\n\\nThe only saving grace was the shredded beef burrito and the Tomatillo-Avocado salsa. No, I will not be going back, even with the Yelp offer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6785adc-47e1-4097-868e-5ad0a5a299a5",
   "metadata": {},
   "source": [
    "### 1.2 数据预处理\n",
    "下载数据集到本地后，使用Tokenizer来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "888e1f60-27df-4278-87e6-a75ed0773ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5031bceb57f4ba7a14e8a3ea58283f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\");\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5aea03-1997-4907-96ed-e2bb0296b5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdb970af-c645-44a5-b168-bd24230e3048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>I wasn't too impressed...I thought I was at the wrong place after going to their website!  I guess I wasn't expecting a bar, and was expecting a more classier place.\\n\\nThat aside, the food was okay.  I had the Caprese salad which was good and the Asparagus was good.  Nothing that wow'ed me.  My friend had some kind of pasta...he didn't like the pasta.  It was okay.\\n\\nCheesecake was good though!!  :) Overall, eh...not my type of thing to dine on but I gave it a try</td>\n",
       "      <td>[101, 146, 1445, 112, 189, 1315, 7351, 119, 119, 119, 146, 1354, 146, 1108, 1120, 1103, 2488, 1282, 1170, 1280, 1106, 1147, 3265, 106, 146, 3319, 146, 1445, 112, 189, 7805, 170, 2927, 117, 1105, 1108, 7805, 170, 1167, 1705, 2852, 1282, 119, 165, 183, 165, 183, 1942, 11220, 4783, 117, 1103, 2094, 1108, 3008, 119, 146, 1125, 1103, 17212, 4894, 1162, 19359, 1134, 1108, 1363, 1105, 1103, 1249, 17482, 28026, 1116, 1108, 1363, 119, 4302, 1115, 192, 4064, 112, 5048, 1143, 119, 1422, 1910, 1125, 1199, 1912, 1104, 1763, 1161, 119, 119, 119, 1119, 1238, 112, 189, 1176, 1103, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 star</td>\n",
       "      <td>1 star for being accommodating and seating us during peak time without a reservation. 1 star for the decent sangaria (I asked for white, but they brought a red one...oh well, it was not bad). The staff was running around the place like crazy. We started with the goat cheese salad, but the cheese was Brie (although the owner tried to assure us it was goat...no way!). Main course was file mignon....I know my steaks and that was at best a sirloin (bad quality sirloin). Well cooked but very chewy and not edible! The other course was an overlooked sole, tasted fresh, but very overlooked. Surprised by the 4-star Yelp rating. Disappointed this was our last dinner in MTL. Had a great dinning experience otherwise at L'express and Bonaparte.</td>\n",
       "      <td>[101, 122, 2851, 1111, 1217, 170, 14566, 6262, 16848, 1916, 1105, 11051, 1366, 1219, 4709, 1159, 1443, 170, 15702, 119, 122, 2851, 1111, 1103, 11858, 6407, 11315, 113, 146, 1455, 1111, 1653, 117, 1133, 1152, 1814, 170, 1894, 1141, 119, 119, 119, 9294, 1218, 117, 1122, 1108, 1136, 2213, 114, 119, 1109, 2546, 1108, 1919, 1213, 1103, 1282, 1176, 4523, 119, 1284, 1408, 1114, 1103, 17497, 9553, 19359, 117, 1133, 1103, 9553, 1108, 139, 5997, 113, 1780, 1103, 3172, 1793, 1106, 14955, 1366, 1122, 1108, 17497, 119, 119, 119, 1185, 1236, 106, 114, 119, 4304, 1736, 1108, 4956, 1940, 25566, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9873a-9b0e-4427-8759-393e18f28150",
   "metadata": {},
   "source": [
    "### 1.3 数据抽样\n",
    "在训练过程中，为了更好地控制训练过程，我们可以从数据集中抽样出一部分数据进行测试。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7a0b421-a0bb-4454-b324-a96cdef4dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从数据集中抽样1000个训练数据集\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed = 42).select(range(1000))\n",
    "\n",
    "# 从数据集中抽样1000个测试样本\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed = 42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46e725a-2b27-4f26-9eb7-0e9f97b7d386",
   "metadata": {},
   "source": [
    "## 2. 微调配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f09dd0-cb9b-4253-9fbd-28c1cc5a8103",
   "metadata": {},
   "source": [
    "### 2.1 训练超参数设置\n",
    "\n",
    "- 完整配置参数与默认值：[https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "\n",
    "- 源代码定义：[https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161](https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ca6055b-4c41-4da5-9538-7d471aa54e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# 模型保存路径\n",
    "output_dir = \"models/bert-base-cased\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    logging_steps = 100  # 每 100 步记录一次日志\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e06a1d-3bd6-4dbd-b75a-ed91f3511ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased/runs/Dec04_10-49-51_hengzq,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=models/bert-base-cased,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe395b-4852-43c1-af7e-7a7b15ac5920",
   "metadata": {},
   "source": [
    "### 2.2 训练评估指标设置(Evaluate)\n",
    "\n",
    "- 完整的评估指标：[https://huggingface.co/evaluate-metric](https://huggingface.co/evaluate-metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29d49cdc-b968-493d-8838-8bfa988a253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71845fa-f32f-4688-a034-f0e617f963c7",
   "metadata": {},
   "source": [
    "### 2.3 训练器配置\n",
    "\n",
    "- 模型首页：[https://huggingface.co/google-bert/bert-base-multilingual-cased](https://huggingface.co/google-bert/bert-base-multilingual-cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6d90bdb-31f1-459b-8517-2099518917d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = small_train_dataset, # 训练数据集\n",
    "    eval_dataset = small_eval_dataset, # 验证数据集\n",
    "    compute_metrics = compute_metrics # 计算指标的函数\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d487851-a32c-4576-a56d-b152aaac9e74",
   "metadata": {},
   "source": [
    "## 3. 模型训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0009d742-51c8-4302-be81-e1dee93d6276",
   "metadata": {},
   "source": [
    "### 3.1 开始训练\n",
    "\n",
    "使用 Trainer 类的 train 方法开始训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80832a72-e6c8-468a-b02e-c1f4704f88e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 02:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.786600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.9836104227701823, metrics={'train_runtime': 163.7982, 'train_samples_per_second': 18.315, 'train_steps_per_second': 2.289, 'total_flos': 789354427392000.0, 'train_loss': 0.9836104227701823, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc55629c-7ac2-4a62-9d10-f83a86f6a23f",
   "metadata": {},
   "source": [
    "**使用 nvidia-smi 监控 GPU 使用**\n",
    "\n",
    "```shell\n",
    "watch -n 1 nvidia-smi\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4060 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
    "| N/A   69C    P0             74W /   78W |    4422MiB /   8188MiB |    100%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|    0   N/A  N/A      3017      C   /python3.10                                 N/A      |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f0626-f704-4887-891d-8c5e8e5d487b",
   "metadata": {},
   "source": [
    "## 4. 保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f2c10-0796-40c7-a13e-eb87fe4ddfed",
   "metadata": {},
   "source": [
    "### 4.1 保存模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e360cc4-1641-497d-b908-ba4bdf0fda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型默认保存到 output_dir = \"models/bert-base-cased\"\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad12d7-2622-4578-81ca-505d7964a9ee",
   "metadata": {},
   "source": [
    "### 4.2 保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d2c3ea1-fb0d-43fd-913d-f104435a9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存路径为：output_dir\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb7216-a590-4018-8f2a-462d15fe9525",
   "metadata": {},
   "source": [
    "## 参考\n",
    "- [https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer](https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa7690-111e-4dc7-bef1-1f60a20fdc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
