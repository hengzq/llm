{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115e031e-c322-4383-8662-9523dc6cb25b",
   "metadata": {},
   "source": [
    "#  Hugging Face Transformers 微调训练\n",
    "\n",
    "- 数据集下载\n",
    "- 数据预处理\n",
    "- 训练超参数设置\n",
    "- 训练评估指标设置\n",
    "- 训练器基本介绍\n",
    "- 实战训练\n",
    "- 模型保存\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e5de8-99d3-4361-ae3f-458472f3a6cf",
   "metadata": {},
   "source": [
    "## 数据集下载\n",
    "\n",
    "    数据集首页:[https://huggingface.co/datasets/Yelp/yelp_review_full](https://huggingface.co/datasets/Yelp/yelp_review_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f9d619-7d7c-4a66-867e-589929040fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_path = \"/home/hengzq/workspace/modelscope/datasets/yelp_review_full\"\n",
    "\n",
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db7185f-42a1-43f8-b964-cb8dea32b7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a089631d-2e4a-497b-8850-24bfbcf7144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': \"Owning a driving range inside the city limits is like a license to print money.  I don't think I ask much out of a driving range.  Decent mats, clean balls and accessible hours.  Hell you need even less people now with the advent of the machine that doles out the balls.  This place has none of them.  It is april and there are no grass tees yet.  BTW they opened for the season this week although it has been golfing weather for a month.  The mats look like the carpet at my 107 year old aunt Irene's house.  Worn and thread bare.  Let's talk about the hours.  This place is equipped with lights yet they only sell buckets of balls until 730.  It is still light out.  Finally lets you have the pit to hit into.  When I arrived I wasn't sure if this was a driving range or an excavation site for a mastodon or a strip mining operation.  There is no grass on the range. Just mud.  Makes it a good tool to figure out how far you actually are hitting the ball.  Oh, they are cash only also.\\\\n\\\\nBottom line, this place sucks.  The best hope is that the owner sells it to someone that actually wants to make money and service golfers in Pittsburgh.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f253d31e-afe2-4d93-b5ce-755298e0ef5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': \"Think Chuck E. Cheese for adults.  Skee Ball, video games pool tables.  Clean environment.  Good fun.\\\\n\\\\nUnfortunately, I went for a bite to eat and it was impossible to find anything good and healthy on the menu.  I ended up settling for spinach dip.  Sadly, they topped the dip off with horrible orange shredded cheese that appeared to have been popped in the microwave for a few seconds.  Blahhhh.  Trying to get something healthy, I ordered the apple pecan salad.  I swear the dressing came right out of the grocery store bottle.  I could barely eat the salad.  Too sweet.\\\\n\\\\nMy mom ordered a steak roll.....holy friedness!  The steak was more like hamburger fried with cheese and then stuffed into breading that was fried AGAIN!  Yowzer!  Artery clogger for sure.  \\\\n\\\\nI like the atmosphere.  I like the bar area.  Perhaps next time we'll just stop by for drinks instead.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90d992c5-657c-4c43-ba8d-6f3e558b63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c658876-d60f-45fc-a203-2abd9907c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples = 10):\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for col, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[col] = df[col].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e0e5b31-fcbe-4103-abcc-7b2573676b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Everything you need in a Sportsbook.  Big TVA, scoreboard and individual seating.  Bar and food options close by, nice and convenient but not very busy which is what I prefer.  If you like the crowds and loud excitement, not a place for you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>When I'm craving Tommy's , this is my spot . I grew up on this in LA and since moving to Vegas from Hawaii , finding Tommy's here has given me a piece of home and I only live down the street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>The two most important words to remember when going here are: Moscow mule.\\n\\nForget everything else; that's what you want. The jukebox has issues, as others have noted. The beer selection is decent, but there is far better in near proximity. The food is decent, but nothing to keep me coming back. What will keep me coming back, however, is the moscow mules. The first few I had there were made with cucumber vodka, and hot damn were they tasty! On another visit, I had a couple made with a vodka infused with mint and I can't even remember what else which one of the bartenders made herself.\\n\\nOh yes, speaking of the bartenders, they are awesome. Be nice to them, treat them well, and they will be awesome to you in return.\\n\\nSo in summary: Food, decent; Beer, decent; Moscow mules, delicious; Bartenders, awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>This is by far my favorite Mexican food in the valley. The tacos aren't massive, but what they lack in size they make up in taste. If you're going for the first time, get the number 5. It's a great way to try a burro, taco, and enchilada (I love the red sauce, my friend loves the green).\\n\\nWhenever I go camping, I pick up burros here and eat them with no food prep mess. My friends are always jealous of my food on those trips.\\n\\nI can't get enough of the chips and salsa. All around, I'm certain you'll love this place.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>This one was the second five guys I've ever been to. The quality and taste were good, but I prefer the location on Lake Mead &amp; Tenaya for cleanliness and friendliness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2 star</td>\n",
       "      <td>She's pretty nice, but got passive aggressive throughout the haircut. \\nShe doesn't know how to cut male Asian hair, it was very mediocre. I was disappointed on multiple occasions hoping she would get it right. \\nIt is convenient how you don't need to make an appointment, its 10 bucks, charges more for things like washing your hair if you have product before you come in.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Quality service at an affordable price.  Puddle Jumper goes out of their way to make the customer happy.  Even after the giant dust storms last summer!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2 star</td>\n",
       "      <td>I came in knowing I would have to either replace my transmission or get it rebuilt. \\n\\nWhen they did their free inspection, they said that they will need to do a more thorough inspection and charged about 280 (not exact but close to that amount). We didn't know the area so we decided to do the inspection to see what it could be. They later on tell us that it was just a simple solenoid. Which wouldn't cost as much as a replacement of a transmission or a rebuilt. \\n\\nWhen they lifted up the car and took the transmission apart, they told us that an motor mount was broken and we should get it fixed. (That meant more money) We got the motor mount fixed. \\n\\nAfter all that was done, the car was still having the same problem and nothing was really fixed. We ended up being out almost 400 and with still the same problem we came in there with. \\n\\nThey said that there is nothing else but to just either get it rebuilt or get a brand new transmission. Getting a rebuilt they would've charged me 2200. I just decided that was just way too much.\\n\\nI went to another place which was a lot cheaper than Paladin. And when I got my car fixed, the mechanic said that one of my solenoid was broken. (That would only mean that when Paladin was fixing my car, they broke my solenoid when they had it cleaned and put back into my car) That cost me more money to fix. =[ \\n\\nI have to admit though, the staff was very friendly and courteous. We didn't put up a fit and just paid them because they did spend the time trying to fix my car. They did try to save me money in hopes it was just a simple solenoid, but in the end . . .it cost me more than save.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"], 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c5ab0-dda1-499e-9bf2-574f7a49b795",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "     下载数据集到本地后，使用Tokenizer来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n",
    "\n",
    "     Datasets的map方法，支持一次性在整个数据集上应用预处理函数。\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd9847fa-6e87-4370-b549-9f5abaf2d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  使用填充到最大长度的策略，处理整个数据集：\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/home/hengzq/workspace/modelscope/models/bert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding = \"max_length\", truncation = True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d73afcd-01ce-4fa9-87ee-d23020573a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Out of all the buffets I've been to in Vegas, this definitely takes the crown of the best right here.\\n\\nSure it costed $30 for a weekend lunch, but I definitely got my $30 worth and whole lot more.  I was especially a fan of Pad Thai, the unlimited number of Spicy Tuna hand rolls, and Ribs!  So delicious!\\n\\nI'd make every effort to come back here the next time I'm in town.  Definitely worth it.</td>\n",
       "      <td>[101, 3929, 1104, 1155, 1103, 171, 9435, 6248, 146, 112, 1396, 1151, 1106, 1107, 6554, 117, 1142, 5397, 2274, 1103, 6371, 1104, 1103, 1436, 1268, 1303, 119, 165, 183, 165, 183, 1708, 3313, 1122, 2616, 1174, 109, 1476, 1111, 170, 5138, 5953, 117, 1133, 146, 5397, 1400, 1139, 109, 1476, 3869, 1105, 2006, 1974, 1167, 119, 146, 1108, 2108, 170, 5442, 1104, 19585, 1181, 6888, 117, 1103, 22921, 1295, 1104, 156, 20437, 1183, 17037, 1605, 1289, 12205, 117, 1105, 155, 13292, 1116, 106, 1573, 13108, 106, 165, 183, 165, 183, 2240, 112, 173, 1294, 1451, 3098, 1106, 1435, 1171, 1303, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Had lunch at Gado Gado today with a large group of 20. We had our own section in the dining room. The place was nice on the inside and the restaurant was about half full. Service was friendly and efficient, especially for a large group.\\n\\nWe ordered 2 appetizers for the table. Rock shrimp and spinach artichoke dip. The Rick shrimp were breaded with a sweet sauce...very good! The spin dip was just ok. For my meal I ordered the veggie flatbread and a cup of their soup of the day, which was mushroom. Soup was lukewarm at best. Too bad, because the flavor was good. The flatbread had large fresh veggies on it with a crispy crust. My only complaint was that it had too much cheddar cheese on it. Wished it was just Italian cheeses.\\n\\nAll in all...A-OK!</td>\n",
       "      <td>[101, 6467, 5953, 1120, 144, 9359, 144, 9359, 2052, 1114, 170, 1415, 1372, 1104, 1406, 119, 1284, 1125, 1412, 1319, 2237, 1107, 1103, 7659, 1395, 119, 1109, 1282, 1108, 3505, 1113, 1103, 1656, 1105, 1103, 4382, 1108, 1164, 1544, 1554, 119, 2516, 1108, 4931, 1105, 7856, 117, 2108, 1111, 170, 1415, 1372, 119, 165, 183, 165, 183, 2924, 1162, 2802, 123, 12647, 26883, 26542, 1111, 1103, 1952, 119, 2977, 23982, 1105, 6898, 7291, 1893, 7255, 9143, 20866, 119, 1109, 5512, 23982, 1127, 8162, 1174, 1114, 170, 4105, 14313, 119, 119, 119, 1304, 1363, 106, 1109, 6898, 20866, 1108, 1198, 21534, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35da3b-d95a-4049-acf2-b6862acee26a",
   "metadata": {},
   "source": [
    "### 数据抽样\n",
    "\n",
    "`shuffle()`函数会随机重新排列列的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64181f26-aafa-43e1-817f-4743fbcd25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed = 42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed = 42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7ac9b-62b2-4dec-b022-a1a03dbcaea4",
   "metadata": {},
   "source": [
    "## 加载模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f6a6fdb-a99d-4ff5-883a-f5faefcf3b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/hengzq/workspace/modelscope/models/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"/home/hengzq/workspace/modelscope/models/bert-base-cased\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249dcc0-20cc-403b-b732-c67edf036d55",
   "metadata": {},
   "source": [
    "## 训练超参数设置\n",
    "\n",
    "完整配置参数与默认值：[https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "\n",
    "源代码定义：[https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161](https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a164f2fa-cd97-4a4b-947e-6c7ffa4f061a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/hengzq/workspace/modelscope/models/bert-base-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# logging_steps 默认值为500，根据我们的训练数据和步长，设置为100\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/test_trainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/test_trainer/runs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/training_args.py:1773\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/training_args.py:2299\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/utils/generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/training_args.py:2172\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2173\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2174\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2175\u001b[0m         )\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"/home/hengzq/workspace/modelscope/models/bert-base-cased\"\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，设置为100\n",
    "training_args = TrainingArguments(output_dir = f\"{model_dir}/test_trainer\",\n",
    "                                  logging_dir = f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3058519-5502-4fad-a203-3197600db7b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/hengzq/workspace/modelscope/models/bert-base-cased/test_trainer/runs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/hengzq/workspace/modelscope/models/bert-base-cased/test_trainer,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/hengzq/workspace/modelscope/models/bert-base-cased/test_trainer,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e71acb-ed07-4e44-a70c-ab7705c767e6",
   "metadata": {},
   "source": [
    "## 训练评估指标设置(Evaluate)\n",
    "\n",
    "    Hugging Face Evaluate库 支持使用一行代码，获得数十种不同领域（自然语言处理，计算机视觉、强化学习等）的评估方法。\n",
    "\n",
    "    完整的评估指标：[https://huggingface.co/evaluate-metric](https://huggingface.co/evaluate-metric)\n",
    "\n",
    "    训练器（Trainer)在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告评估。\n",
    "\n",
    "    Evaluate库提供了一个简单的准确率函数，你可以使用`evaluate.load`函数加载\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40923fc2-346c-480c-a36a-bca0f396cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  定义度量函数\n",
    "def compute_accuracy(predictions, references):\n",
    "    correct = sum(p == r for p, r in zip(predictions, references))\n",
    "    return correct / len(predictions) if len(predictions) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d74d08-97dc-4782-b15a-62bb43575a31",
   "metadata": {},
   "source": [
    "### 训练过程指标监控\n",
    "    通常，为了监控训练过程中的评估指标变化，我们可以在`TrainingArguments`指定`evaluation_strategy`参数，以便在epoch结束时评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becadf1-e703-4f9e-9873-0e4893dcd9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir = f\"{model_dir}/test_trainer\",\n",
    "                                  logging_dir = f\"{model_dir}/test_trainer/runs\",\n",
    "                                  evaluation_strategy = \"epoch\",\n",
    "                                  logging_steps = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187ce02-5162-4da8-a87b-330f80705496",
   "metadata": {},
   "source": [
    "## 训练器基本介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c57bad-c572-47e4-8889-1808d198ffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5f35619-b984-46e7-b9d4-b9092f08b3dc",
   "metadata": {},
   "source": [
    "## 实战训练\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52267ac4-f8ae-4d97-80e6-2bcaead0c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = small_eval_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4dc3d-c4ac-46cc-8b57-1e3da3a580a2",
   "metadata": {},
   "source": [
    "### 训练状态\n",
    "- 使用`trainer.save_state`方法保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515e638-6271-4aa6-ba2c-6f202d82649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae756fd-8a43-419c-81fc-6138afdf4074",
   "metadata": {},
   "source": [
    "## 模型保存\n",
    "\n",
    "- 使用`trainer.save_model`方法保存模型，后续可以通过`from_pretrained()`方法重新加载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180efc8-bd75-4f3f-a2c8-2de1256e6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
