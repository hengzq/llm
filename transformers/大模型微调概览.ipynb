{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d275deb-fc6f-4538-8338-38659b6a7270",
   "metadata": {},
   "source": [
    "# 大模型微调概览\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f3d353-92b9-46be-b7a7-fc5a9a06fb46",
   "metadata": {},
   "source": [
    "## 微调模型概览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04ceae-499f-4046-99c5-8ff694ac5553",
   "metadata": {},
   "source": [
    "## 微调数据概览\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05d50e-46c9-4dbb-bd86-6dc2c4cae49c",
   "metadata": {},
   "source": [
    "## 微调技术概览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4be25-0a09-471d-aa82-87ca429e83a3",
   "metadata": {},
   "source": [
    "# Lora 微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c823e80c-6b7a-490e-be46-89b18b2c0ea9",
   "metadata": {},
   "source": [
    "## 怎么区分预训练和微调？\n",
    "\n",
    "**预训练(Pre-Training)**：是指在大规模数据集上先对模型进行训练，以学习一些通用的特征表示或知识。在自然语言处理领域，预训练通常是指在大型文本语料库上训练模型，例如GPT模型就是在大规模文本数据上进行预训练的。预训练的目的是为了让模型学习到足够通用的语言表示，以便后续在特定任务上进行微调。\n",
    "\n",
    "**微调(Fine-tuning)**:是指在预训练好的模型基础上，针对特定的任务或领域，使用较小规模的数据集对模型进行进一步的训练。微调的目的是为了让模型适应特定任务的数据分布和特征，使其在特定任务上表现更好。在自然语言处理领域，微调通常包括在预训练模型之上添加额外的输出层，并使用特定任务的数据集对整个模型进行训练。\n",
    "\n",
    "因此，预训练是为了在大规模数据上学习通用的特征表示或知识，而微调则是在预训练好的模型基础上，针对特定任务进行进一步训练，使其适应特定任务的需求。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2033aa-73ed-4f2d-82a8-00af004451aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
